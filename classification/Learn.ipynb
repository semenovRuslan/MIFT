{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5110)\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "#path = \"data/state/\"\n",
    "path = \"data/state/sample/\"\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = 'data/botles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2688 images belonging to 4 classes.\n",
      "Found 672 images belonging to 4 classes.\n",
      "Found 57 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "test_batches = get_batches(path+'test', batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2688 images belonging to 4 classes.\n",
      "Found 672 images belonging to 4 classes.\n",
      "Found 57 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames,\n",
    "    test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagenet conv features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have so little data, and it is similar to imagenet images (full color photos), using pre-trained VGG weights is likely to be helpful - in fact it seems likely that we won't need to fine-tune the convolutional layer weights much, if at all. So we can pre-compute the output of the last convolutional layer, as we did in lesson 3 when we experimented with dropout. (However this means that we can't use full data augmentation, since we can't pre-compute something that changes every image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model=vgg.model\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "                                                                   lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "                                                                   zeropadding2d_1[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "                                                                   convolution2d_1[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "                                                                   zeropadding2d_2[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "                                                                   convolution2d_2[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "                                                                   maxpooling2d_1[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "                                                                   zeropadding2d_3[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "                                                                   convolution2d_3[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "                                                                   zeropadding2d_4[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "                                                                   convolution2d_4[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "                                                                   maxpooling2d_2[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "                                                                   zeropadding2d_5[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "                                                                   convolution2d_5[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "                                                                   zeropadding2d_6[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "                                                                   convolution2d_6[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "                                                                   zeropadding2d_7[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "                                                                   convolution2d_7[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "                                                                   maxpooling2d_3[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "                                                                   zeropadding2d_8[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "                                                                   convolution2d_8[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "                                                                   zeropadding2d_9[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "                                                                   convolution2d_9[1][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "                                                                   zeropadding2d_10[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "                                                                   convolution2d_10[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "                                                                   maxpooling2d_4[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "                                                                   zeropadding2d_11[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "                                                                   convolution2d_11[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "                                                                   zeropadding2d_12[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "                                                                   convolution2d_12[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "                                                                   zeropadding2d_13[1][0]           \n",
      "====================================================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2688 images belonging to 4 classes.\n",
      "Found 672 images belonging to 4 classes.\n",
      "Found 57 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#так как мы больше не можем работать с Data augmentation мы должны перезабрать наши масивы заново\n",
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# формируем наши фичи на основе предрасчитаного vgg imagenet\n",
    "conv_feat = conv_model.predict_generator(batches, batches.nb_sample)\n",
    "conv_val_feat = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170L, 512L, 14L, 14L)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#проверим что это выход нашей сети предрасчитанной. Кол во фичей должно быть аналогично к выходу \n",
    "#последнего conv layer  у нас на summary() это convolution2d_41 (Convolution2D)=512L, 14L, 14L\n",
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm dense layers on pretrained conv layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've pre-computed the output of the last convolutional layer, we need to create a network that takes that as input, and predicts our 4 classes. Let's try using a simplified version of VGG's dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Let's try pre-computing 5 epochs worth of augmented data, so we can experiment with combining dropout and augmentation on the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've now got a dataset 6x bigger than before, we'll need to copy our labels 6 times too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on some experiments the previous model works well, with bigger dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_da_layers(p))\n",
    "bn_model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40320 samples, validate on 672 samples\n",
      "Epoch 1/4\n",
      "40320/40320 [==============================] - 26s - loss: 0.8001 - acc: 0.7049 - val_loss: 0.4479 - val_acc: 0.8438\n",
      "Epoch 2/4\n",
      "40320/40320 [==============================] - 27s - loss: 0.6890 - acc: 0.7440 - val_loss: 0.4209 - val_acc: 0.8482\n",
      "Epoch 3/4\n",
      "40320/40320 [==============================] - 27s - loss: 0.6193 - acc: 0.7727 - val_loss: 0.3934 - val_acc: 0.8586\n",
      "Epoch 4/4\n",
      "40320/40320 [==============================] - 27s - loss: 0.5514 - acc: 0.7963 - val_loss: 0.3663 - val_acc: 0.8676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x670c7128>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40320 samples, validate on 672 samples\n",
      "Epoch 1/4\n",
      "40320/40320 [==============================] - 26s - loss: 0.3125 - acc: 0.8878 - val_loss: 0.2967 - val_acc: 0.9062\n",
      "Epoch 2/4\n",
      "40320/40320 [==============================] - 27s - loss: 0.3090 - acc: 0.8890 - val_loss: 0.2996 - val_acc: 0.9033\n",
      "Epoch 3/4\n",
      "40320/40320 [==============================] - 26s - loss: 0.2925 - acc: 0.8958 - val_loss: 0.2975 - val_acc: 0.9018\n",
      "Epoch 4/4\n",
      "40320/40320 [==============================] - 26s - loss: 0.2923 - acc: 0.8978 - val_loss: 0.3008 - val_acc: 0.9033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7395dd30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=4, \n",
    "             validation_data=(conv_val_feat, val_labels))\n",
    "# Результат должен быть acc:0.86 на val_ac:0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40320 samples, validate on 672 samples\n",
      "Epoch 1/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.2895 - acc: 0.8982 - val_loss: 0.2963 - val_acc: 0.9033\n",
      "Epoch 2/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2816 - acc: 0.9005 - val_loss: 0.2937 - val_acc: 0.9033\n",
      "Epoch 3/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.2762 - acc: 0.9028 - val_loss: 0.2973 - val_acc: 0.9003\n",
      "Epoch 4/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2718 - acc: 0.9048 - val_loss: 0.2895 - val_acc: 0.9048\n",
      "Epoch 5/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.2628 - acc: 0.9069 - val_loss: 0.2906 - val_acc: 0.9092\n",
      "Epoch 6/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.2650 - acc: 0.9070 - val_loss: 0.2983 - val_acc: 0.9033\n",
      "Epoch 7/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2679 - acc: 0.9052 - val_loss: 0.2976 - val_acc: 0.9033\n",
      "Epoch 8/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2585 - acc: 0.9097 - val_loss: 0.3019 - val_acc: 0.9048\n",
      "Epoch 9/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2583 - acc: 0.9099 - val_loss: 0.3002 - val_acc: 0.9062\n",
      "Epoch 10/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.2501 - acc: 0.9118 - val_loss: 0.3066 - val_acc: 0.9033\n",
      "Epoch 11/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2473 - acc: 0.9138 - val_loss: 0.2976 - val_acc: 0.9062\n",
      "Epoch 12/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.2507 - acc: 0.9126 - val_loss: 0.2944 - val_acc: 0.9062\n",
      "Epoch 13/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.2405 - acc: 0.9159 - val_loss: 0.2926 - val_acc: 0.9062\n",
      "Epoch 14/256\n",
      "40320/40320 [==============================] - 29s - loss: 0.2358 - acc: 0.9166 - val_loss: 0.2958 - val_acc: 0.9062\n",
      "Epoch 15/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.2308 - acc: 0.9190 - val_loss: 0.2912 - val_acc: 0.9107\n",
      "Epoch 16/256\n",
      "40320/40320 [==============================] - 22s - loss: 0.2357 - acc: 0.9172 - val_loss: 0.2888 - val_acc: 0.9137\n",
      "Epoch 17/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.2344 - acc: 0.9198 - val_loss: 0.2913 - val_acc: 0.9122\n",
      "Epoch 18/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.2259 - acc: 0.9206 - val_loss: 0.2909 - val_acc: 0.9033\n",
      "Epoch 19/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2256 - acc: 0.9214 - val_loss: 0.2916 - val_acc: 0.9033\n",
      "Epoch 20/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.2248 - acc: 0.9210 - val_loss: 0.2869 - val_acc: 0.9092\n",
      "Epoch 21/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2241 - acc: 0.9207 - val_loss: 0.2887 - val_acc: 0.9018\n",
      "Epoch 22/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2186 - acc: 0.9235 - val_loss: 0.2930 - val_acc: 0.9107\n",
      "Epoch 23/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.2206 - acc: 0.9232 - val_loss: 0.2975 - val_acc: 0.9048\n",
      "Epoch 24/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.2115 - acc: 0.9263 - val_loss: 0.2943 - val_acc: 0.9062\n",
      "Epoch 25/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.2164 - acc: 0.9243 - val_loss: 0.2924 - val_acc: 0.9077\n",
      "Epoch 26/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.2094 - acc: 0.9268 - val_loss: 0.2939 - val_acc: 0.9077\n",
      "Epoch 27/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.2129 - acc: 0.9275 - val_loss: 0.3010 - val_acc: 0.9062\n",
      "Epoch 28/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.2075 - acc: 0.9289 - val_loss: 0.2957 - val_acc: 0.9062\n",
      "Epoch 29/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.2059 - acc: 0.9284 - val_loss: 0.2983 - val_acc: 0.9122\n",
      "Epoch 30/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.2037 - acc: 0.9281 - val_loss: 0.2979 - val_acc: 0.9092\n",
      "Epoch 31/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1985 - acc: 0.9317 - val_loss: 0.2970 - val_acc: 0.8988\n",
      "Epoch 32/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1990 - acc: 0.9320 - val_loss: 0.3020 - val_acc: 0.9033\n",
      "Epoch 33/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.2032 - acc: 0.9300 - val_loss: 0.2900 - val_acc: 0.9062\n",
      "Epoch 34/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.2028 - acc: 0.9302 - val_loss: 0.2899 - val_acc: 0.9077\n",
      "Epoch 35/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1956 - acc: 0.9330 - val_loss: 0.2895 - val_acc: 0.9092\n",
      "Epoch 36/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1918 - acc: 0.9331 - val_loss: 0.2863 - val_acc: 0.9077\n",
      "Epoch 37/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1946 - acc: 0.9323 - val_loss: 0.2909 - val_acc: 0.9077\n",
      "Epoch 38/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1903 - acc: 0.9340 - val_loss: 0.2865 - val_acc: 0.9092\n",
      "Epoch 39/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1879 - acc: 0.9355 - val_loss: 0.2933 - val_acc: 0.9077\n",
      "Epoch 40/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1974 - acc: 0.9319 - val_loss: 0.2895 - val_acc: 0.9062\n",
      "Epoch 41/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1853 - acc: 0.9345 - val_loss: 0.2949 - val_acc: 0.9033\n",
      "Epoch 42/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1868 - acc: 0.9352 - val_loss: 0.2832 - val_acc: 0.9092\n",
      "Epoch 43/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1864 - acc: 0.9348 - val_loss: 0.2922 - val_acc: 0.9062\n",
      "Epoch 44/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1821 - acc: 0.9365 - val_loss: 0.2861 - val_acc: 0.9018\n",
      "Epoch 45/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1871 - acc: 0.9354 - val_loss: 0.2801 - val_acc: 0.9122\n",
      "Epoch 46/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1838 - acc: 0.9366 - val_loss: 0.2816 - val_acc: 0.9107\n",
      "Epoch 47/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1795 - acc: 0.9384 - val_loss: 0.2881 - val_acc: 0.9137\n",
      "Epoch 48/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1798 - acc: 0.9377 - val_loss: 0.2890 - val_acc: 0.9077\n",
      "Epoch 49/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1767 - acc: 0.9380 - val_loss: 0.2820 - val_acc: 0.9137\n",
      "Epoch 50/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1769 - acc: 0.9378 - val_loss: 0.2915 - val_acc: 0.9077\n",
      "Epoch 51/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1758 - acc: 0.9403 - val_loss: 0.2805 - val_acc: 0.9077\n",
      "Epoch 52/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1773 - acc: 0.9394 - val_loss: 0.2816 - val_acc: 0.9107\n",
      "Epoch 53/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1759 - acc: 0.9384 - val_loss: 0.2903 - val_acc: 0.9048\n",
      "Epoch 54/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1718 - acc: 0.9393 - val_loss: 0.2853 - val_acc: 0.9122\n",
      "Epoch 55/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1710 - acc: 0.9416 - val_loss: 0.2841 - val_acc: 0.9062\n",
      "Epoch 56/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1722 - acc: 0.9411 - val_loss: 0.2893 - val_acc: 0.9048\n",
      "Epoch 57/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1701 - acc: 0.9414 - val_loss: 0.2922 - val_acc: 0.9137\n",
      "Epoch 58/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1658 - acc: 0.9423 - val_loss: 0.2824 - val_acc: 0.9152\n",
      "Epoch 59/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1689 - acc: 0.9421 - val_loss: 0.2783 - val_acc: 0.9137\n",
      "Epoch 60/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1650 - acc: 0.9429 - val_loss: 0.2929 - val_acc: 0.9107\n",
      "Epoch 61/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1676 - acc: 0.9426 - val_loss: 0.2861 - val_acc: 0.9137\n",
      "Epoch 62/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1615 - acc: 0.9452 - val_loss: 0.2815 - val_acc: 0.9137\n",
      "Epoch 63/256\n",
      "40320/40320 [==============================] - 23s - loss: 0.1683 - acc: 0.9428 - val_loss: 0.2838 - val_acc: 0.9137\n",
      "Epoch 64/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1626 - acc: 0.9450 - val_loss: 0.2835 - val_acc: 0.9092\n",
      "Epoch 65/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1624 - acc: 0.9436 - val_loss: 0.2933 - val_acc: 0.9092\n",
      "Epoch 66/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1577 - acc: 0.9450 - val_loss: 0.2929 - val_acc: 0.9062\n",
      "Epoch 67/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1621 - acc: 0.9448 - val_loss: 0.2879 - val_acc: 0.9077\n",
      "Epoch 68/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1579 - acc: 0.9456 - val_loss: 0.2880 - val_acc: 0.9077\n",
      "Epoch 69/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1606 - acc: 0.9444 - val_loss: 0.2888 - val_acc: 0.9077\n",
      "Epoch 70/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1582 - acc: 0.9459 - val_loss: 0.2827 - val_acc: 0.9152\n",
      "Epoch 71/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1571 - acc: 0.9459 - val_loss: 0.2944 - val_acc: 0.9062\n",
      "Epoch 72/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1513 - acc: 0.9482 - val_loss: 0.2911 - val_acc: 0.9077\n",
      "Epoch 73/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1539 - acc: 0.9472 - val_loss: 0.2864 - val_acc: 0.9107\n",
      "Epoch 74/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1566 - acc: 0.9473 - val_loss: 0.2914 - val_acc: 0.9122\n",
      "Epoch 75/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1561 - acc: 0.9465 - val_loss: 0.2806 - val_acc: 0.9137\n",
      "Epoch 76/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1520 - acc: 0.9486 - val_loss: 0.2900 - val_acc: 0.9107\n",
      "Epoch 77/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1496 - acc: 0.9494 - val_loss: 0.2867 - val_acc: 0.9092\n",
      "Epoch 78/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1503 - acc: 0.9491 - val_loss: 0.2863 - val_acc: 0.9107\n",
      "Epoch 79/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1543 - acc: 0.9479 - val_loss: 0.2805 - val_acc: 0.9137\n",
      "Epoch 80/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1454 - acc: 0.9495 - val_loss: 0.2905 - val_acc: 0.9092\n",
      "Epoch 81/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1505 - acc: 0.9494 - val_loss: 0.2842 - val_acc: 0.9092\n",
      "Epoch 82/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1483 - acc: 0.9506 - val_loss: 0.2898 - val_acc: 0.9077\n",
      "Epoch 83/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1477 - acc: 0.9502 - val_loss: 0.2794 - val_acc: 0.9122\n",
      "Epoch 84/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1478 - acc: 0.9500 - val_loss: 0.2792 - val_acc: 0.9122\n",
      "Epoch 85/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1516 - acc: 0.9494 - val_loss: 0.2886 - val_acc: 0.9048\n",
      "Epoch 86/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1458 - acc: 0.9501 - val_loss: 0.2958 - val_acc: 0.9048\n",
      "Epoch 87/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1446 - acc: 0.9508 - val_loss: 0.2873 - val_acc: 0.9137\n",
      "Epoch 88/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1402 - acc: 0.9528 - val_loss: 0.2859 - val_acc: 0.9107\n",
      "Epoch 89/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1516 - acc: 0.9484 - val_loss: 0.2838 - val_acc: 0.9152\n",
      "Epoch 90/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1442 - acc: 0.9506 - val_loss: 0.2853 - val_acc: 0.9152\n",
      "Epoch 91/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1420 - acc: 0.9517 - val_loss: 0.2866 - val_acc: 0.9167\n",
      "Epoch 92/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1398 - acc: 0.9529 - val_loss: 0.2905 - val_acc: 0.9122\n",
      "Epoch 93/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1494 - acc: 0.9498 - val_loss: 0.2791 - val_acc: 0.9152\n",
      "Epoch 94/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1439 - acc: 0.9501 - val_loss: 0.2836 - val_acc: 0.9122\n",
      "Epoch 95/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1351 - acc: 0.9542 - val_loss: 0.2871 - val_acc: 0.9122\n",
      "Epoch 96/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1414 - acc: 0.9528 - val_loss: 0.2886 - val_acc: 0.9107\n",
      "Epoch 97/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1400 - acc: 0.9527 - val_loss: 0.2903 - val_acc: 0.9092\n",
      "Epoch 98/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1402 - acc: 0.9522 - val_loss: 0.2844 - val_acc: 0.9137\n",
      "Epoch 99/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1411 - acc: 0.9528 - val_loss: 0.2867 - val_acc: 0.9152\n",
      "Epoch 100/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1429 - acc: 0.9516 - val_loss: 0.2937 - val_acc: 0.9092\n",
      "Epoch 101/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1350 - acc: 0.9538 - val_loss: 0.2917 - val_acc: 0.9137\n",
      "Epoch 102/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1382 - acc: 0.9534 - val_loss: 0.2888 - val_acc: 0.9122\n",
      "Epoch 103/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1360 - acc: 0.9539 - val_loss: 0.2844 - val_acc: 0.9211\n",
      "Epoch 104/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1366 - acc: 0.9529 - val_loss: 0.2876 - val_acc: 0.9152\n",
      "Epoch 105/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1350 - acc: 0.9533 - val_loss: 0.2905 - val_acc: 0.9196\n",
      "Epoch 106/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1360 - acc: 0.9537 - val_loss: 0.2939 - val_acc: 0.9152\n",
      "Epoch 107/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1375 - acc: 0.9541 - val_loss: 0.2946 - val_acc: 0.9167\n",
      "Epoch 108/256\n",
      "40320/40320 [==============================] - 30s - loss: 0.1356 - acc: 0.9540 - val_loss: 0.2953 - val_acc: 0.9092\n",
      "Epoch 109/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1358 - acc: 0.9544 - val_loss: 0.2972 - val_acc: 0.9092\n",
      "Epoch 110/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1337 - acc: 0.9545 - val_loss: 0.2965 - val_acc: 0.9092\n",
      "Epoch 111/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1357 - acc: 0.9554 - val_loss: 0.2914 - val_acc: 0.9122\n",
      "Epoch 112/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1271 - acc: 0.9556 - val_loss: 0.2976 - val_acc: 0.9137\n",
      "Epoch 113/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1338 - acc: 0.9546 - val_loss: 0.2965 - val_acc: 0.9152\n",
      "Epoch 114/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1300 - acc: 0.9559 - val_loss: 0.2938 - val_acc: 0.9107\n",
      "Epoch 115/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1304 - acc: 0.9555 - val_loss: 0.3019 - val_acc: 0.9062\n",
      "Epoch 116/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1337 - acc: 0.9554 - val_loss: 0.3069 - val_acc: 0.9033\n",
      "Epoch 117/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1277 - acc: 0.9565 - val_loss: 0.2978 - val_acc: 0.9122\n",
      "Epoch 118/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1347 - acc: 0.9546 - val_loss: 0.2988 - val_acc: 0.9092\n",
      "Epoch 119/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1338 - acc: 0.9545 - val_loss: 0.2961 - val_acc: 0.9122\n",
      "Epoch 120/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1298 - acc: 0.9554 - val_loss: 0.2982 - val_acc: 0.9122\n",
      "Epoch 121/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1285 - acc: 0.9563 - val_loss: 0.3041 - val_acc: 0.9122\n",
      "Epoch 122/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1246 - acc: 0.9574 - val_loss: 0.2999 - val_acc: 0.9107\n",
      "Epoch 123/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1291 - acc: 0.9564 - val_loss: 0.2984 - val_acc: 0.9107\n",
      "Epoch 124/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1297 - acc: 0.9562 - val_loss: 0.3000 - val_acc: 0.9122\n",
      "Epoch 125/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1274 - acc: 0.9584 - val_loss: 0.2976 - val_acc: 0.9152\n",
      "Epoch 126/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1250 - acc: 0.9575 - val_loss: 0.3060 - val_acc: 0.9107\n",
      "Epoch 127/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1261 - acc: 0.9567 - val_loss: 0.3042 - val_acc: 0.9152\n",
      "Epoch 128/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1293 - acc: 0.9558 - val_loss: 0.3003 - val_acc: 0.9182\n",
      "Epoch 129/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1292 - acc: 0.9568 - val_loss: 0.2987 - val_acc: 0.9152\n",
      "Epoch 130/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1247 - acc: 0.9587 - val_loss: 0.3051 - val_acc: 0.9092\n",
      "Epoch 131/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1272 - acc: 0.9564 - val_loss: 0.2962 - val_acc: 0.9137\n",
      "Epoch 132/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1278 - acc: 0.9572 - val_loss: 0.2995 - val_acc: 0.9152\n",
      "Epoch 133/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1246 - acc: 0.9577 - val_loss: 0.3025 - val_acc: 0.9137\n",
      "Epoch 134/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1185 - acc: 0.9595 - val_loss: 0.3016 - val_acc: 0.9137\n",
      "Epoch 135/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1270 - acc: 0.9568 - val_loss: 0.3035 - val_acc: 0.9152\n",
      "Epoch 136/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1216 - acc: 0.9584 - val_loss: 0.3021 - val_acc: 0.9152\n",
      "Epoch 137/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1226 - acc: 0.9586 - val_loss: 0.3038 - val_acc: 0.9122\n",
      "Epoch 138/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1251 - acc: 0.9586 - val_loss: 0.2954 - val_acc: 0.9167\n",
      "Epoch 139/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1240 - acc: 0.9586 - val_loss: 0.2992 - val_acc: 0.9152\n",
      "Epoch 140/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1211 - acc: 0.9594 - val_loss: 0.2947 - val_acc: 0.9167\n",
      "Epoch 141/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1246 - acc: 0.9585 - val_loss: 0.3005 - val_acc: 0.9122\n",
      "Epoch 142/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1226 - acc: 0.9583 - val_loss: 0.3032 - val_acc: 0.9092\n",
      "Epoch 143/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1183 - acc: 0.9596 - val_loss: 0.3060 - val_acc: 0.9077\n",
      "Epoch 144/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1216 - acc: 0.9595 - val_loss: 0.2999 - val_acc: 0.9122\n",
      "Epoch 145/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1229 - acc: 0.9586 - val_loss: 0.3068 - val_acc: 0.9092\n",
      "Epoch 146/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1208 - acc: 0.9591 - val_loss: 0.3041 - val_acc: 0.9062\n",
      "Epoch 147/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1239 - acc: 0.9582 - val_loss: 0.2980 - val_acc: 0.9152\n",
      "Epoch 148/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1214 - acc: 0.9591 - val_loss: 0.2970 - val_acc: 0.9092\n",
      "Epoch 149/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1216 - acc: 0.9585 - val_loss: 0.2976 - val_acc: 0.9107\n",
      "Epoch 150/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1165 - acc: 0.9615 - val_loss: 0.2904 - val_acc: 0.9122\n",
      "Epoch 151/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1216 - acc: 0.9585 - val_loss: 0.2906 - val_acc: 0.9122\n",
      "Epoch 152/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1193 - acc: 0.9604 - val_loss: 0.2869 - val_acc: 0.9167\n",
      "Epoch 153/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1189 - acc: 0.9599 - val_loss: 0.2846 - val_acc: 0.9137\n",
      "Epoch 154/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1159 - acc: 0.9615 - val_loss: 0.2894 - val_acc: 0.9182\n",
      "Epoch 155/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1142 - acc: 0.9609 - val_loss: 0.2914 - val_acc: 0.9137\n",
      "Epoch 156/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1186 - acc: 0.9597 - val_loss: 0.2877 - val_acc: 0.9122\n",
      "Epoch 157/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1186 - acc: 0.9602 - val_loss: 0.2916 - val_acc: 0.9122\n",
      "Epoch 158/256\n",
      "40320/40320 [==============================] - 30s - loss: 0.1169 - acc: 0.9602 - val_loss: 0.2993 - val_acc: 0.9107\n",
      "Epoch 159/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1225 - acc: 0.9589 - val_loss: 0.2878 - val_acc: 0.9152\n",
      "Epoch 160/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1123 - acc: 0.9629 - val_loss: 0.2950 - val_acc: 0.9122\n",
      "Epoch 161/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1119 - acc: 0.9624 - val_loss: 0.3026 - val_acc: 0.9077\n",
      "Epoch 162/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1167 - acc: 0.9611 - val_loss: 0.3039 - val_acc: 0.9107\n",
      "Epoch 163/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1149 - acc: 0.9616 - val_loss: 0.2938 - val_acc: 0.9152\n",
      "Epoch 164/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1125 - acc: 0.9625 - val_loss: 0.2893 - val_acc: 0.9182\n",
      "Epoch 165/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1141 - acc: 0.9618 - val_loss: 0.2962 - val_acc: 0.9137\n",
      "Epoch 166/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1143 - acc: 0.9618 - val_loss: 0.2948 - val_acc: 0.9152\n",
      "Epoch 167/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1142 - acc: 0.9618 - val_loss: 0.2953 - val_acc: 0.9137\n",
      "Epoch 168/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1144 - acc: 0.9616 - val_loss: 0.2918 - val_acc: 0.9152\n",
      "Epoch 169/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1130 - acc: 0.9621 - val_loss: 0.2878 - val_acc: 0.9167\n",
      "Epoch 170/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1107 - acc: 0.9620 - val_loss: 0.2932 - val_acc: 0.9137\n",
      "Epoch 171/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1122 - acc: 0.9622 - val_loss: 0.2962 - val_acc: 0.9137\n",
      "Epoch 172/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1088 - acc: 0.9627 - val_loss: 0.2934 - val_acc: 0.9137\n",
      "Epoch 173/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1117 - acc: 0.9623 - val_loss: 0.2977 - val_acc: 0.9152\n",
      "Epoch 174/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1108 - acc: 0.9625 - val_loss: 0.2920 - val_acc: 0.9182\n",
      "Epoch 175/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1119 - acc: 0.9612 - val_loss: 0.2929 - val_acc: 0.9152\n",
      "Epoch 176/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1124 - acc: 0.9618 - val_loss: 0.2898 - val_acc: 0.9182\n",
      "Epoch 177/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1114 - acc: 0.9631 - val_loss: 0.2894 - val_acc: 0.9152\n",
      "Epoch 178/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1076 - acc: 0.9634 - val_loss: 0.2936 - val_acc: 0.9196\n",
      "Epoch 179/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1119 - acc: 0.9630 - val_loss: 0.2961 - val_acc: 0.9182\n",
      "Epoch 180/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1096 - acc: 0.9636 - val_loss: 0.2885 - val_acc: 0.9167\n",
      "Epoch 181/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1116 - acc: 0.9630 - val_loss: 0.2870 - val_acc: 0.9152\n",
      "Epoch 182/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1077 - acc: 0.9634 - val_loss: 0.2858 - val_acc: 0.9167\n",
      "Epoch 183/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1101 - acc: 0.9634 - val_loss: 0.2962 - val_acc: 0.9152\n",
      "Epoch 184/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1086 - acc: 0.9630 - val_loss: 0.2986 - val_acc: 0.9122\n",
      "Epoch 185/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1065 - acc: 0.9645 - val_loss: 0.3013 - val_acc: 0.9137\n",
      "Epoch 186/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1101 - acc: 0.9637 - val_loss: 0.2930 - val_acc: 0.9122\n",
      "Epoch 187/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1120 - acc: 0.9618 - val_loss: 0.2847 - val_acc: 0.9196\n",
      "Epoch 188/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1126 - acc: 0.9633 - val_loss: 0.2840 - val_acc: 0.9167\n",
      "Epoch 189/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1090 - acc: 0.9638 - val_loss: 0.2888 - val_acc: 0.9167\n",
      "Epoch 190/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1092 - acc: 0.9638 - val_loss: 0.2945 - val_acc: 0.9122\n",
      "Epoch 191/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1105 - acc: 0.9628 - val_loss: 0.2895 - val_acc: 0.9152\n",
      "Epoch 192/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1094 - acc: 0.9636 - val_loss: 0.2943 - val_acc: 0.9152\n",
      "Epoch 193/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1041 - acc: 0.9640 - val_loss: 0.2990 - val_acc: 0.9152\n",
      "Epoch 194/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1092 - acc: 0.9633 - val_loss: 0.2940 - val_acc: 0.9167\n",
      "Epoch 195/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1068 - acc: 0.9648 - val_loss: 0.2900 - val_acc: 0.9167\n",
      "Epoch 196/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1052 - acc: 0.9647 - val_loss: 0.3005 - val_acc: 0.9152\n",
      "Epoch 197/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1084 - acc: 0.9648 - val_loss: 0.2956 - val_acc: 0.9226\n",
      "Epoch 198/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1099 - acc: 0.9642 - val_loss: 0.2942 - val_acc: 0.9167\n",
      "Epoch 199/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1060 - acc: 0.9648 - val_loss: 0.2981 - val_acc: 0.9167\n",
      "Epoch 200/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1027 - acc: 0.9655 - val_loss: 0.3025 - val_acc: 0.9167\n",
      "Epoch 201/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1065 - acc: 0.9641 - val_loss: 0.2955 - val_acc: 0.9182\n",
      "Epoch 202/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1097 - acc: 0.9633 - val_loss: 0.2935 - val_acc: 0.9196\n",
      "Epoch 203/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1049 - acc: 0.9649 - val_loss: 0.2942 - val_acc: 0.9167\n",
      "Epoch 204/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1082 - acc: 0.9643 - val_loss: 0.2897 - val_acc: 0.9167\n",
      "Epoch 205/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1018 - acc: 0.9664 - val_loss: 0.2918 - val_acc: 0.9226\n",
      "Epoch 206/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1033 - acc: 0.9656 - val_loss: 0.2952 - val_acc: 0.9167\n",
      "Epoch 207/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1042 - acc: 0.9660 - val_loss: 0.2965 - val_acc: 0.9182\n",
      "Epoch 208/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1050 - acc: 0.9647 - val_loss: 0.2892 - val_acc: 0.9211\n",
      "Epoch 209/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1054 - acc: 0.9640 - val_loss: 0.2908 - val_acc: 0.9196\n",
      "Epoch 210/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1087 - acc: 0.9632 - val_loss: 0.2813 - val_acc: 0.9241\n",
      "Epoch 211/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1062 - acc: 0.9644 - val_loss: 0.2876 - val_acc: 0.9226\n",
      "Epoch 212/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1068 - acc: 0.9630 - val_loss: 0.2843 - val_acc: 0.9256\n",
      "Epoch 213/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1023 - acc: 0.9668 - val_loss: 0.2900 - val_acc: 0.9226\n",
      "Epoch 214/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1072 - acc: 0.9648 - val_loss: 0.2877 - val_acc: 0.9226\n",
      "Epoch 215/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1019 - acc: 0.9660 - val_loss: 0.2869 - val_acc: 0.9226\n",
      "Epoch 216/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1022 - acc: 0.9654 - val_loss: 0.2951 - val_acc: 0.9226\n",
      "Epoch 217/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.1062 - acc: 0.9642 - val_loss: 0.2932 - val_acc: 0.9226\n",
      "Epoch 218/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1016 - acc: 0.9660 - val_loss: 0.2931 - val_acc: 0.9211\n",
      "Epoch 219/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1074 - acc: 0.9646 - val_loss: 0.2920 - val_acc: 0.9226\n",
      "Epoch 220/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1045 - acc: 0.9654 - val_loss: 0.2960 - val_acc: 0.9182\n",
      "Epoch 221/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.0999 - acc: 0.9659 - val_loss: 0.2963 - val_acc: 0.9211\n",
      "Epoch 222/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1040 - acc: 0.9650 - val_loss: 0.2893 - val_acc: 0.9196\n",
      "Epoch 223/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1015 - acc: 0.9661 - val_loss: 0.2916 - val_acc: 0.9167\n",
      "Epoch 224/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.0973 - acc: 0.9674 - val_loss: 0.2881 - val_acc: 0.9226\n",
      "Epoch 225/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1021 - acc: 0.9667 - val_loss: 0.2944 - val_acc: 0.9182\n",
      "Epoch 226/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.0997 - acc: 0.9673 - val_loss: 0.2934 - val_acc: 0.9211\n",
      "Epoch 227/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1022 - acc: 0.9666 - val_loss: 0.2931 - val_acc: 0.9196\n",
      "Epoch 228/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1021 - acc: 0.9670 - val_loss: 0.2934 - val_acc: 0.9182\n",
      "Epoch 229/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1023 - acc: 0.9661 - val_loss: 0.2907 - val_acc: 0.9196\n",
      "Epoch 230/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1030 - acc: 0.9657 - val_loss: 0.2847 - val_acc: 0.9211\n",
      "Epoch 231/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1032 - acc: 0.9655 - val_loss: 0.2904 - val_acc: 0.9196\n",
      "Epoch 232/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1025 - acc: 0.9647 - val_loss: 0.2861 - val_acc: 0.9226\n",
      "Epoch 233/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1024 - acc: 0.9660 - val_loss: 0.2875 - val_acc: 0.9211\n",
      "Epoch 234/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.0978 - acc: 0.9672 - val_loss: 0.2901 - val_acc: 0.9241\n",
      "Epoch 235/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1007 - acc: 0.9660 - val_loss: 0.2947 - val_acc: 0.9211\n",
      "Epoch 236/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.0943 - acc: 0.9682 - val_loss: 0.2881 - val_acc: 0.9226\n",
      "Epoch 237/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.0958 - acc: 0.9678 - val_loss: 0.2873 - val_acc: 0.9241\n",
      "Epoch 238/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.0994 - acc: 0.9657 - val_loss: 0.2937 - val_acc: 0.9241\n",
      "Epoch 239/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1013 - acc: 0.9654 - val_loss: 0.2923 - val_acc: 0.9226\n",
      "Epoch 240/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1007 - acc: 0.9667 - val_loss: 0.2893 - val_acc: 0.9196\n",
      "Epoch 241/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.1006 - acc: 0.9657 - val_loss: 0.2872 - val_acc: 0.9241\n",
      "Epoch 242/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.1013 - acc: 0.9664 - val_loss: 0.2964 - val_acc: 0.9182\n",
      "Epoch 243/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.1010 - acc: 0.9669 - val_loss: 0.2884 - val_acc: 0.9256\n",
      "Epoch 244/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.0977 - acc: 0.9668 - val_loss: 0.2918 - val_acc: 0.9241\n",
      "Epoch 245/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.0994 - acc: 0.9672 - val_loss: 0.2845 - val_acc: 0.9241\n",
      "Epoch 246/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.0938 - acc: 0.9686 - val_loss: 0.2878 - val_acc: 0.9256\n",
      "Epoch 247/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.0982 - acc: 0.9685 - val_loss: 0.2855 - val_acc: 0.9256\n",
      "Epoch 248/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.0982 - acc: 0.9666 - val_loss: 0.2920 - val_acc: 0.9196\n",
      "Epoch 249/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.0931 - acc: 0.9691 - val_loss: 0.2904 - val_acc: 0.9211\n",
      "Epoch 250/256\n",
      "40320/40320 [==============================] - 23s - loss: 0.1002 - acc: 0.9668 - val_loss: 0.2911 - val_acc: 0.9196\n",
      "Epoch 251/256\n",
      "40320/40320 [==============================] - 25s - loss: 0.0947 - acc: 0.9692 - val_loss: 0.2884 - val_acc: 0.9211\n",
      "Epoch 252/256\n",
      "40320/40320 [==============================] - 24s - loss: 0.1003 - acc: 0.9674 - val_loss: 0.2938 - val_acc: 0.9196\n",
      "Epoch 253/256\n",
      "40320/40320 [==============================] - 28s - loss: 0.0950 - acc: 0.9674 - val_loss: 0.2886 - val_acc: 0.9211\n",
      "Epoch 254/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.0978 - acc: 0.9684 - val_loss: 0.2872 - val_acc: 0.9211\n",
      "Epoch 255/256\n",
      "40320/40320 [==============================] - 26s - loss: 0.0928 - acc: 0.9688 - val_loss: 0.2888 - val_acc: 0.9226\n",
      "Epoch 256/256\n",
      "40320/40320 [==============================] - 27s - loss: 0.0998 - acc: 0.9669 - val_loss: 0.2912 - val_acc: 0.9211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0c51588>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=256, \n",
    "             validation_data=(conv_val_feat, val_labels))\n",
    "# Результат должен быть acc:0.89 на val_ac:0.81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good - let's save those weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'results/da_conv8_1_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'results/da_conv8_1_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 512, 7, 7)     0           maxpooling2d_input_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 25088)         0           maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 25088)         0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 256)           6422784     dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 256)           1024        dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256)           0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           65792       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 256)           1024        dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 256)           0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4)             1028        dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 6,491,652\n",
      "Trainable params: 6,490,628\n",
      "Non-trainable params: 1,024\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find a good clipping amount using the validation set, prior to submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c49c79881ea6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_clip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.93\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'val_labels' is not defined"
     ]
    }
   ],
   "source": [
    "keras.metrics.categorical_crossentropy(val_labels, do_clip(val_preds, 0.93)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_test_feat = load_array(path+'results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bn_model.predict(conv_test_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(preds,0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm_name = path+'results/subm.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0jpg.jpg</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.jpg</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.jpg</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.jpg</td>\n",
       "      <td>0.030927</td>\n",
       "      <td>0.032437</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.jpg</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.jpg</td>\n",
       "      <td>0.084884</td>\n",
       "      <td>0.911997</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.jpg</td>\n",
       "      <td>0.544594</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.444635</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.032004</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>0.555725</td>\n",
       "      <td>0.011257</td>\n",
       "      <td>0.431975</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.015442</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>212.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.035174</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23421.JPG</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24.jpg</td>\n",
       "      <td>0.925489</td>\n",
       "      <td>0.071535</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>26.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>27.jpg</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>0.728799</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>28.jpg</td>\n",
       "      <td>0.758690</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.229744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>282.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>29.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.178980</td>\n",
       "      <td>0.814129</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30.jpg</td>\n",
       "      <td>0.723251</td>\n",
       "      <td>0.195433</td>\n",
       "      <td>0.073741</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31.jpg</td>\n",
       "      <td>0.708645</td>\n",
       "      <td>0.284850</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32.jpg</td>\n",
       "      <td>0.288912</td>\n",
       "      <td>0.016808</td>\n",
       "      <td>0.686507</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34.jpg</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>0.834037</td>\n",
       "      <td>0.152171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36.jpg</td>\n",
       "      <td>0.215833</td>\n",
       "      <td>0.783813</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38.jpg</td>\n",
       "      <td>0.460437</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.538816</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41.jpg</td>\n",
       "      <td>0.288484</td>\n",
       "      <td>0.692010</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42.jpg</td>\n",
       "      <td>0.812247</td>\n",
       "      <td>0.166849</td>\n",
       "      <td>0.020014</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>0.025965</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45.jpg</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.018625</td>\n",
       "      <td>0.031793</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.jpg</td>\n",
       "      <td>0.010866</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>0.196757</td>\n",
       "      <td>0.776892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>7.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>8.jpg</td>\n",
       "      <td>0.018096</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.jpg</td>\n",
       "      <td>0.093230</td>\n",
       "      <td>0.024008</td>\n",
       "      <td>0.880146</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>asdfdasf.JPG</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>bottles-wine-4765121.jpg</td>\n",
       "      <td>0.767871</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.010454</td>\n",
       "      <td>0.220350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>dfkj.JPG</td>\n",
       "      <td>0.306225</td>\n",
       "      <td>0.679969</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>drgfasf.JPG</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>dsfgdfsg.JPG</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.015429</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>lghdfkjs.jpg</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.007778</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.007778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         img         1         2         3         4\n",
       "0                      0.jpg  0.930000  0.010243  0.007778  0.007778\n",
       "1                   0jpg.jpg  0.930000  0.007778  0.007778  0.007778\n",
       "2                      1.jpg  0.007778  0.930000  0.007778  0.007778\n",
       "3                     10.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "4                     11.jpg  0.930000  0.007778  0.007778  0.007778\n",
       "5                     12.jpg  0.930000  0.007778  0.009513  0.007778\n",
       "6                     13.jpg  0.030927  0.032437  0.930000  0.007778\n",
       "7                     14.jpg  0.015829  0.930000  0.007778  0.007778\n",
       "8                     15.jpg  0.084884  0.911997  0.007778  0.007778\n",
       "9                     16.jpg  0.544594  0.007778  0.444635  0.007778\n",
       "10                    17.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "11                    18.jpg  0.007778  0.007778  0.032004  0.930000\n",
       "12                    19.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "13                     2.jpg  0.555725  0.011257  0.431975  0.007778\n",
       "14                    20.jpg  0.007778  0.007778  0.015442  0.930000\n",
       "15                    21.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "16                   212.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "17                    22.jpg  0.007778  0.007778  0.035174  0.930000\n",
       "18                    23.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "19                 23421.JPG  0.007778  0.007778  0.007778  0.930000\n",
       "20                    24.jpg  0.925489  0.071535  0.007778  0.007778\n",
       "21                    25.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "22                    26.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "23                    27.jpg  0.265823  0.728799  0.007778  0.007778\n",
       "24                    28.jpg  0.758690  0.007778  0.007778  0.229744\n",
       "25                   282.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "26                    29.jpg  0.007778  0.007778  0.930000  0.007778\n",
       "27                     3.jpg  0.007778  0.178980  0.814129  0.007778\n",
       "28                    30.jpg  0.723251  0.195433  0.073741  0.007778\n",
       "29                    31.jpg  0.708645  0.284850  0.007778  0.007778\n",
       "30                    32.jpg  0.288912  0.016808  0.686507  0.007778\n",
       "31                    33.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "32                    34.jpg  0.930000  0.007778  0.007778  0.007778\n",
       "33                    35.jpg  0.007778  0.008754  0.834037  0.152171\n",
       "34                    36.jpg  0.215833  0.783813  0.007778  0.007778\n",
       "35                    37.jpg  0.007778  0.007778  0.930000  0.007778\n",
       "36                    38.jpg  0.460437  0.007778  0.538816  0.007778\n",
       "37                    39.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "38                     4.jpg  0.007778  0.007778  0.014154  0.930000\n",
       "39                    40.jpg  0.007778  0.007778  0.930000  0.007778\n",
       "40                    41.jpg  0.288484  0.692010  0.017285  0.007778\n",
       "41                    42.jpg  0.812247  0.166849  0.020014  0.007778\n",
       "42                    43.jpg  0.007778  0.021439  0.025965  0.930000\n",
       "43                    44.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "44                    45.jpg  0.930000  0.018625  0.031793  0.007778\n",
       "45                    46.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "46                     5.jpg  0.010866  0.015486  0.196757  0.776892\n",
       "47                     6.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "48                     7.jpg  0.007778  0.007778  0.007778  0.930000\n",
       "49                     8.jpg  0.018096  0.007778  0.930000  0.007778\n",
       "50                     9.jpg  0.093230  0.024008  0.880146  0.007778\n",
       "51              asdfdasf.JPG  0.007778  0.007778  0.007778  0.930000\n",
       "52  bottles-wine-4765121.jpg  0.767871  0.007778  0.010454  0.220350\n",
       "53                  dfkj.JPG  0.306225  0.679969  0.008054  0.007778\n",
       "54               drgfasf.JPG  0.007778  0.930000  0.007778  0.007778\n",
       "55              dsfgdfsg.JPG  0.930000  0.015429  0.007778  0.007778\n",
       "56              lghdfkjs.jpg  0.007778  0.007778  0.930000  0.007778"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[4:] for a in test_filenames])\n",
    "submission#.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57L, 512L, 14L, 14L)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_test_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receivig full model where productivity would be same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = Sequential(conv_layers)\n",
    "bn_layers = bn_model.layers #get_bn_da_layers(p)\n",
    "for layer in final_model.layers: layer.trainable = False\n",
    "for layer in bn_layers: \n",
    "    final_model.add(layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.compile(Adam(), 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[9][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[9][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[9][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[9][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[9][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[9][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[9][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[9][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[9][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[9][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[9][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[9][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 25088)         0           maxpooling2d_6[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 25088)         0           flatten_2[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 256)           6422784     dropout_3[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 256)           1024        dense_4[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256)           0           batchnormalization_1[1][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 256)           65792       dropout_4[1][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 256)           1024        dense_5[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 256)           0           batchnormalization_2[1][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4)             1028        dropout_5[1][0]                  \n",
      "====================================================================================================\n",
      "Total params: 21,206,340\n",
      "Trainable params: 6,490,628\n",
      "Non-trainable params: 14,715,712\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.7304e-04   5.4656e-04   8.3965e-04   9.9834e-01]]\n",
      "Class is [3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fa5916a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAAD8CAYAAACIPKEAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5Ph/nM6/AvtlWgIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f80fab70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.io import imread\n",
    "import cv2\n",
    "\n",
    "\n",
    "#original=imread(\"vodka.jpg\")#sham1#cog#vodka\n",
    "#s = cv2.resize(original, (224, 224))\n",
    "#sm=s.transpose(2, 0, 1)\n",
    "\n",
    "original=imread(path+'test/100/39.jpg')#sham1#cog#vodka\n",
    "\n",
    "s = cv2.resize(original, (224, 224))\n",
    "s=s.transpose(2, 0, 1)\n",
    "a=s.reshape((1, s.shape[0], s.shape[1], s.shape[2]))\n",
    "\n",
    "prediction = final_model.predict(a)\n",
    "print (prediction)\n",
    "print(\"Class is {}\".format(np.argmax(prediction, axis=1)))\n",
    "plt.imshow(original, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.save_weights(path+'results/final_20170101.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:terano]",
   "language": "python",
   "name": "conda-env-terano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "nav_menu": {
    "height": "148px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
